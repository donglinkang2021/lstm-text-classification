{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tokenizer import Vocabulary\n",
    "vocab = Vocabulary.load(\"./data/process/vocab.txt\")\n",
    "label_list = ['财经', '彩票', '房产', '股票', '家居', '教育', '科技', '社会', '时尚', '时政', '体育', '星座', '游戏', '娱乐']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import NewsDataset, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset = NewsDataset(\"./data/process/train.txt\")\n",
    "val_dataset = NewsDataset(\"./data/process/val.txt\")\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============train_loader =============\n",
      "tensor([[ 941,  343, 1940,   53,   49, 3476, 2090, 2144, 1842,  225, 1005,   51,\n",
      "           74,   61,   23,   18, 1646,   11,   48,    9,   12,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [  23,  166,   52, 2274,  145,   10,  589,  364,  963, 1131,  178,  986,\n",
      "         2193,  336,   13, 2274,  101,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [1230,   49, 2007,   73,  374,  659,  354,   33, 1431,    2,   74,  374,\n",
      "         1276, 1421,  488, 2000, 2008,  113,  136,   83,  327,  131,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   6,    3,    4,    3,   17,  116,  313,   15,  251,  178,  824,   67,\n",
      "          283,  136,  376,   52,  111,   11,    9,   12,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "torch.Size([4, 32])\n",
      "tensor([ 7,  9, 13,  5])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print('=============train_loader =============')\n",
    "for data, label in train_loader:\n",
    "    print(data)\n",
    "    print(data.shape)\n",
    "    print(label)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.rnn import RNN\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "num_layers = 1\n",
    "num_classes = len(label_list)\n",
    "model = RNN(vocab_size, embedding_dim, hidden_dim, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "RNN                                      [2, 14]                   --\n",
       "├─Embedding: 1-1                         [2, 14, 128]              672,128\n",
       "├─LSTM: 1-2                              [2, 14, 128]              132,096\n",
       "├─Linear: 1-3                            [2, 14]                   1,806\n",
       "==========================================================================================\n",
       "Total params: 806,030\n",
       "Trainable params: 806,030\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 5.05\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.06\n",
       "Params size (MB): 3.22\n",
       "Estimated Total Size (MB): 3.28\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.LongTensor([\n",
    "    [1,2,3,4,5,6,7,8,9,10,11,12,13,19],\n",
    "    [1,2,3,4,5,6,7,8,9,10,11,12,13,19],\n",
    "])\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model, input_data=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, dataloader, device):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            y_pred.extend(torch.argmax(out, dim=-1).tolist())\n",
    "            y_true.extend(y.tolist())\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [20/23515], Loss: 2.0458\n",
      "Epoch [1/10], Step [40/23515], Loss: 1.8403\n",
      "Epoch [1/10], Step [60/23515], Loss: 3.7955\n",
      "Epoch [1/10], Step [80/23515], Loss: 2.3585\n",
      "Epoch [1/10], Step [100/23515], Loss: 1.7105\n",
      "Epoch [1/10], Step [120/23515], Loss: 1.8702\n",
      "Epoch [1/10], Step [140/23515], Loss: 2.6521\n",
      "Epoch [1/10], Step [160/23515], Loss: 2.5443\n",
      "Epoch [1/10], Step [180/23515], Loss: 2.6538\n",
      "Epoch [1/10], Step [200/23515], Loss: 2.2453\n",
      "Epoch [1/10], Step [220/23515], Loss: 1.9674\n",
      "Epoch [1/10], Step [240/23515], Loss: 2.2733\n",
      "Epoch [1/10], Step [260/23515], Loss: 2.3976\n",
      "Epoch [1/10], Step [280/23515], Loss: 2.0486\n",
      "Epoch [1/10], Step [300/23515], Loss: 2.1470\n",
      "Epoch [1/10], Step [320/23515], Loss: 2.3423\n",
      "Epoch [1/10], Step [340/23515], Loss: 2.3574\n",
      "Epoch [1/10], Step [360/23515], Loss: 2.5475\n",
      "Epoch [1/10], Step [380/23515], Loss: 2.2412\n",
      "Epoch [1/10], Step [400/23515], Loss: 2.7647\n",
      "Epoch [1/10], Step [420/23515], Loss: 2.1533\n",
      "Epoch [1/10], Step [440/23515], Loss: 1.9607\n",
      "Epoch [1/10], Step [460/23515], Loss: 2.6946\n",
      "Epoch [1/10], Step [480/23515], Loss: 1.5105\n",
      "Epoch [1/10], Step [500/23515], Loss: 2.3925\n",
      "Epoch [1/10], Step [520/23515], Loss: 2.4810\n",
      "Epoch [1/10], Step [540/23515], Loss: 2.6354\n",
      "Epoch [1/10], Step [560/23515], Loss: 1.8960\n",
      "Epoch [1/10], Step [580/23515], Loss: 1.8092\n",
      "Epoch [1/10], Step [600/23515], Loss: 1.8939\n",
      "Epoch [1/10], Step [620/23515], Loss: 2.9106\n",
      "Epoch [1/10], Step [640/23515], Loss: 2.2259\n",
      "Epoch [1/10], Step [660/23515], Loss: 1.9826\n",
      "Epoch [1/10], Step [680/23515], Loss: 1.9419\n",
      "Epoch [1/10], Step [700/23515], Loss: 2.2937\n",
      "Epoch [1/10], Step [720/23515], Loss: 1.4706\n",
      "Epoch [1/10], Step [740/23515], Loss: 2.4187\n",
      "Epoch [1/10], Step [760/23515], Loss: 1.6272\n",
      "Epoch [1/10], Step [780/23515], Loss: 1.1173\n",
      "Epoch [1/10], Step [800/23515], Loss: 1.4800\n",
      "Epoch [1/10], Step [820/23515], Loss: 3.3669\n",
      "Epoch [1/10], Step [840/23515], Loss: 0.9994\n",
      "Epoch [1/10], Step [860/23515], Loss: 1.8199\n",
      "Epoch [1/10], Step [880/23515], Loss: 2.6638\n",
      "Epoch [1/10], Step [900/23515], Loss: 2.0306\n",
      "Epoch [1/10], Step [920/23515], Loss: 2.2069\n",
      "Epoch [1/10], Step [940/23515], Loss: 1.4612\n",
      "Epoch [1/10], Step [960/23515], Loss: 2.7534\n",
      "Epoch [1/10], Step [980/23515], Loss: 2.2566\n",
      "Epoch [1/10], Step [1000/23515], Loss: 2.0064\n",
      "Epoch [1/10], Step [1020/23515], Loss: 1.4275\n",
      "Epoch [1/10], Step [1040/23515], Loss: 1.4925\n",
      "Epoch [1/10], Step [1060/23515], Loss: 2.5176\n",
      "Epoch [1/10], Step [1080/23515], Loss: 1.3367\n",
      "Epoch [1/10], Step [1100/23515], Loss: 1.2721\n",
      "Epoch [1/10], Step [1120/23515], Loss: 1.9955\n",
      "Epoch [1/10], Step [1140/23515], Loss: 0.7919\n",
      "Epoch [1/10], Step [1160/23515], Loss: 1.3850\n",
      "Epoch [1/10], Step [1180/23515], Loss: 2.6600\n",
      "Epoch [1/10], Step [1200/23515], Loss: 2.5161\n",
      "Epoch [1/10], Step [1220/23515], Loss: 1.7526\n",
      "Epoch [1/10], Step [1240/23515], Loss: 1.7878\n",
      "Epoch [1/10], Step [1260/23515], Loss: 1.8415\n",
      "Epoch [1/10], Step [1280/23515], Loss: 0.8608\n",
      "Epoch [1/10], Step [1300/23515], Loss: 2.4188\n",
      "Epoch [1/10], Step [1320/23515], Loss: 1.8523\n",
      "Epoch [1/10], Step [1340/23515], Loss: 1.4207\n",
      "Epoch [1/10], Step [1360/23515], Loss: 1.6190\n",
      "Epoch [1/10], Step [1380/23515], Loss: 1.7280\n",
      "Epoch [1/10], Step [1400/23515], Loss: 1.7960\n",
      "Epoch [1/10], Step [1420/23515], Loss: 1.3653\n",
      "Epoch [1/10], Step [1440/23515], Loss: 1.2261\n",
      "Epoch [1/10], Step [1460/23515], Loss: 2.1571\n",
      "Epoch [1/10], Step [1480/23515], Loss: 1.6498\n",
      "Epoch [1/10], Step [1500/23515], Loss: 1.0751\n",
      "Epoch [1/10], Step [1520/23515], Loss: 1.9546\n",
      "Epoch [1/10], Step [1540/23515], Loss: 1.0723\n",
      "Epoch [1/10], Step [1560/23515], Loss: 1.3945\n",
      "Epoch [1/10], Step [1580/23515], Loss: 1.1759\n",
      "Epoch [1/10], Step [1600/23515], Loss: 2.2653\n",
      "Epoch [1/10], Step [1620/23515], Loss: 1.4445\n",
      "Epoch [1/10], Step [1640/23515], Loss: 1.1333\n",
      "Epoch [1/10], Step [1660/23515], Loss: 1.2501\n",
      "Epoch [1/10], Step [1680/23515], Loss: 2.2065\n",
      "Epoch [1/10], Step [1700/23515], Loss: 1.0132\n",
      "Epoch [1/10], Step [1720/23515], Loss: 1.2453\n",
      "Epoch [1/10], Step [1740/23515], Loss: 1.5028\n",
      "Epoch [1/10], Step [1760/23515], Loss: 1.0097\n",
      "Epoch [1/10], Step [1780/23515], Loss: 0.9218\n",
      "Epoch [1/10], Step [1800/23515], Loss: 1.5931\n",
      "Epoch [1/10], Step [1820/23515], Loss: 0.3031\n",
      "Epoch [1/10], Step [1840/23515], Loss: 0.9284\n",
      "Epoch [1/10], Step [1860/23515], Loss: 0.6543\n",
      "Epoch [1/10], Step [1880/23515], Loss: 1.1827\n",
      "Epoch [1/10], Step [1900/23515], Loss: 1.5462\n",
      "Epoch [1/10], Step [1920/23515], Loss: 1.5481\n",
      "Epoch [1/10], Step [1940/23515], Loss: 1.6142\n",
      "Epoch [1/10], Step [1960/23515], Loss: 1.7049\n",
      "Epoch [1/10], Step [1980/23515], Loss: 1.4127\n",
      "Epoch [1/10], Step [2000/23515], Loss: 1.2913\n",
      "Epoch [1/10], Step [2020/23515], Loss: 0.7551\n",
      "Epoch [1/10], Step [2040/23515], Loss: 0.9978\n",
      "Epoch [1/10], Step [2060/23515], Loss: 1.0306\n",
      "Epoch [1/10], Step [2080/23515], Loss: 1.7712\n",
      "Epoch [1/10], Step [2100/23515], Loss: 1.0811\n",
      "Epoch [1/10], Step [2120/23515], Loss: 1.5852\n",
      "Epoch [1/10], Step [2140/23515], Loss: 1.7788\n",
      "Epoch [1/10], Step [2160/23515], Loss: 1.3207\n",
      "Epoch [1/10], Step [2180/23515], Loss: 1.1908\n",
      "Epoch [1/10], Step [2200/23515], Loss: 1.8021\n",
      "Epoch [1/10], Step [2220/23515], Loss: 2.9040\n",
      "Epoch [1/10], Step [2240/23515], Loss: 1.9413\n",
      "Epoch [1/10], Step [2260/23515], Loss: 2.0988\n",
      "Epoch [1/10], Step [2280/23515], Loss: 0.8572\n",
      "Epoch [1/10], Step [2300/23515], Loss: 2.8279\n",
      "Epoch [1/10], Step [2320/23515], Loss: 1.4212\n",
      "Epoch [1/10], Step [2340/23515], Loss: 2.3070\n",
      "Epoch [1/10], Step [2360/23515], Loss: 1.3043\n",
      "Epoch [1/10], Step [2380/23515], Loss: 1.1144\n",
      "Epoch [1/10], Step [2400/23515], Loss: 2.4404\n",
      "Epoch [1/10], Step [2420/23515], Loss: 2.0738\n",
      "Epoch [1/10], Step [2440/23515], Loss: 1.2006\n",
      "Epoch [1/10], Step [2460/23515], Loss: 2.2937\n",
      "Epoch [1/10], Step [2480/23515], Loss: 0.8841\n",
      "Epoch [1/10], Step [2500/23515], Loss: 2.3506\n",
      "Epoch [1/10], Step [2520/23515], Loss: 1.6794\n",
      "Epoch [1/10], Step [2540/23515], Loss: 1.9856\n",
      "Epoch [1/10], Step [2560/23515], Loss: 0.2558\n",
      "Epoch [1/10], Step [2580/23515], Loss: 1.0909\n",
      "Epoch [1/10], Step [2600/23515], Loss: 1.9198\n",
      "Epoch [1/10], Step [2620/23515], Loss: 0.2450\n",
      "Epoch [1/10], Step [2640/23515], Loss: 2.4375\n",
      "Epoch [1/10], Step [2660/23515], Loss: 2.0325\n",
      "Epoch [1/10], Step [2680/23515], Loss: 1.4290\n",
      "Epoch [1/10], Step [2700/23515], Loss: 1.0409\n",
      "Epoch [1/10], Step [2720/23515], Loss: 0.9190\n",
      "Epoch [1/10], Step [2740/23515], Loss: 0.6133\n",
      "Epoch [1/10], Step [2760/23515], Loss: 1.1613\n",
      "Epoch [1/10], Step [2780/23515], Loss: 1.5497\n",
      "Epoch [1/10], Step [2800/23515], Loss: 2.0067\n",
      "Epoch [1/10], Step [2820/23515], Loss: 1.4841\n",
      "Epoch [1/10], Step [2840/23515], Loss: 0.7445\n",
      "Epoch [1/10], Step [2860/23515], Loss: 0.8731\n",
      "Epoch [1/10], Step [2880/23515], Loss: 0.6569\n",
      "Epoch [1/10], Step [2900/23515], Loss: 1.4346\n",
      "Epoch [1/10], Step [2920/23515], Loss: 2.0989\n",
      "Epoch [1/10], Step [2940/23515], Loss: 1.0682\n",
      "Epoch [1/10], Step [2960/23515], Loss: 0.8751\n",
      "Epoch [1/10], Step [2980/23515], Loss: 0.7674\n",
      "Epoch [1/10], Step [3000/23515], Loss: 0.3211\n",
      "Epoch [1/10], Step [3020/23515], Loss: 1.1053\n",
      "Epoch [1/10], Step [3040/23515], Loss: 1.2415\n",
      "Epoch [1/10], Step [3060/23515], Loss: 1.1799\n",
      "Epoch [1/10], Step [3080/23515], Loss: 0.7189\n",
      "Epoch [1/10], Step [3100/23515], Loss: 1.2027\n",
      "Epoch [1/10], Step [3120/23515], Loss: 0.5086\n",
      "Epoch [1/10], Step [3140/23515], Loss: 1.0112\n",
      "Epoch [1/10], Step [3160/23515], Loss: 1.1092\n",
      "Epoch [1/10], Step [3180/23515], Loss: 0.2675\n",
      "Epoch [1/10], Step [3200/23515], Loss: 0.8672\n",
      "Epoch [1/10], Step [3220/23515], Loss: 0.5054\n",
      "Epoch [1/10], Step [3240/23515], Loss: 1.2214\n",
      "Epoch [1/10], Step [3260/23515], Loss: 1.2086\n",
      "Epoch [1/10], Step [3280/23515], Loss: 1.1562\n",
      "Epoch [1/10], Step [3300/23515], Loss: 0.9471\n",
      "Epoch [1/10], Step [3320/23515], Loss: 2.3315\n",
      "Epoch [1/10], Step [3340/23515], Loss: 0.9313\n",
      "Epoch [1/10], Step [3360/23515], Loss: 1.0903\n",
      "Epoch [1/10], Step [3380/23515], Loss: 2.5929\n",
      "Epoch [1/10], Step [3400/23515], Loss: 1.0470\n",
      "Epoch [1/10], Step [3420/23515], Loss: 1.1917\n",
      "Epoch [1/10], Step [3440/23515], Loss: 1.1919\n",
      "Epoch [1/10], Step [3460/23515], Loss: 1.1703\n",
      "Epoch [1/10], Step [3480/23515], Loss: 1.6166\n",
      "Epoch [1/10], Step [3500/23515], Loss: 0.5887\n",
      "Epoch [1/10], Step [3520/23515], Loss: 0.8768\n",
      "Epoch [1/10], Step [3540/23515], Loss: 2.2323\n",
      "Epoch [1/10], Step [3560/23515], Loss: 1.6870\n",
      "Epoch [1/10], Step [3580/23515], Loss: 2.2989\n",
      "Epoch [1/10], Step [3600/23515], Loss: 1.5215\n",
      "Epoch [1/10], Step [3620/23515], Loss: 1.2423\n",
      "Epoch [1/10], Step [3640/23515], Loss: 0.7103\n",
      "Epoch [1/10], Step [3660/23515], Loss: 0.5775\n",
      "Epoch [1/10], Step [3680/23515], Loss: 1.3309\n",
      "Epoch [1/10], Step [3700/23515], Loss: 1.0666\n",
      "Epoch [1/10], Step [3720/23515], Loss: 1.0767\n",
      "Epoch [1/10], Step [3740/23515], Loss: 0.4732\n",
      "Epoch [1/10], Step [3760/23515], Loss: 2.3358\n",
      "Epoch [1/10], Step [3780/23515], Loss: 0.9554\n",
      "Epoch [1/10], Step [3800/23515], Loss: 1.2756\n",
      "Epoch [1/10], Step [3820/23515], Loss: 1.3043\n",
      "Epoch [1/10], Step [3840/23515], Loss: 1.5777\n",
      "Epoch [1/10], Step [3860/23515], Loss: 0.7023\n",
      "Epoch [1/10], Step [3880/23515], Loss: 1.9213\n",
      "Epoch [1/10], Step [3900/23515], Loss: 0.3178\n",
      "Epoch [1/10], Step [3920/23515], Loss: 1.6588\n",
      "Epoch [1/10], Step [3940/23515], Loss: 0.3011\n",
      "Epoch [1/10], Step [3960/23515], Loss: 0.7191\n",
      "Epoch [1/10], Step [3980/23515], Loss: 1.4171\n",
      "Epoch [1/10], Step [4000/23515], Loss: 1.1285\n",
      "Epoch [1/10], Step [4020/23515], Loss: 1.0136\n",
      "Epoch [1/10], Step [4040/23515], Loss: 1.1412\n",
      "Epoch [1/10], Step [4060/23515], Loss: 0.2182\n",
      "Epoch [1/10], Step [4080/23515], Loss: 0.2686\n",
      "Epoch [1/10], Step [4100/23515], Loss: 0.3900\n",
      "Epoch [1/10], Step [4120/23515], Loss: 2.8921\n",
      "Epoch [1/10], Step [4140/23515], Loss: 1.3320\n",
      "Epoch [1/10], Step [4160/23515], Loss: 1.6566\n",
      "Epoch [1/10], Step [4180/23515], Loss: 0.6196\n",
      "Epoch [1/10], Step [4200/23515], Loss: 1.6016\n",
      "Epoch [1/10], Step [4220/23515], Loss: 1.0535\n",
      "Epoch [1/10], Step [4240/23515], Loss: 0.5212\n",
      "Epoch [1/10], Step [4260/23515], Loss: 0.6674\n",
      "Epoch [1/10], Step [4280/23515], Loss: 0.3098\n",
      "Epoch [1/10], Step [4300/23515], Loss: 0.8761\n",
      "Epoch [1/10], Step [4320/23515], Loss: 0.9580\n",
      "Epoch [1/10], Step [4340/23515], Loss: 0.7790\n",
      "Epoch [1/10], Step [4360/23515], Loss: 0.5270\n",
      "Epoch [1/10], Step [4380/23515], Loss: 1.6927\n",
      "Epoch [1/10], Step [4400/23515], Loss: 0.3913\n",
      "Epoch [1/10], Step [4420/23515], Loss: 0.6857\n",
      "Epoch [1/10], Step [4440/23515], Loss: 0.7272\n",
      "Epoch [1/10], Step [4460/23515], Loss: 1.4950\n",
      "Epoch [1/10], Step [4480/23515], Loss: 0.9351\n",
      "Epoch [1/10], Step [4500/23515], Loss: 1.0676\n",
      "Epoch [1/10], Step [4520/23515], Loss: 1.9072\n",
      "Epoch [1/10], Step [4540/23515], Loss: 2.0292\n",
      "Epoch [1/10], Step [4560/23515], Loss: 1.8023\n",
      "Epoch [1/10], Step [4580/23515], Loss: 1.1328\n",
      "Epoch [1/10], Step [4600/23515], Loss: 0.9252\n",
      "Epoch [1/10], Step [4620/23515], Loss: 1.2301\n",
      "Epoch [1/10], Step [4640/23515], Loss: 0.9862\n",
      "Epoch [1/10], Step [4660/23515], Loss: 0.5601\n",
      "Epoch [1/10], Step [4680/23515], Loss: 0.6971\n",
      "Epoch [1/10], Step [4700/23515], Loss: 0.5428\n",
      "Epoch [1/10], Step [4720/23515], Loss: 2.6063\n",
      "Epoch [1/10], Step [4740/23515], Loss: 0.5935\n",
      "Epoch [1/10], Step [4760/23515], Loss: 1.4104\n",
      "Epoch [1/10], Step [4780/23515], Loss: 0.3991\n",
      "Epoch [1/10], Step [4800/23515], Loss: 1.2800\n",
      "Epoch [1/10], Step [4820/23515], Loss: 1.2065\n",
      "Epoch [1/10], Step [4840/23515], Loss: 0.6174\n",
      "Epoch [1/10], Step [4860/23515], Loss: 0.9768\n",
      "Epoch [1/10], Step [4880/23515], Loss: 1.1813\n",
      "Epoch [1/10], Step [4900/23515], Loss: 1.6019\n",
      "Epoch [1/10], Step [4920/23515], Loss: 0.2424\n",
      "Epoch [1/10], Step [4940/23515], Loss: 1.6512\n",
      "Epoch [1/10], Step [4960/23515], Loss: 0.5593\n",
      "Epoch [1/10], Step [4980/23515], Loss: 0.8225\n",
      "Epoch [1/10], Step [5000/23515], Loss: 0.7483\n",
      "Epoch [1/10], Step [5020/23515], Loss: 1.4125\n",
      "Epoch [1/10], Step [5040/23515], Loss: 0.4277\n",
      "Epoch [1/10], Step [5060/23515], Loss: 1.2695\n",
      "Epoch [1/10], Step [5080/23515], Loss: 0.4629\n",
      "Epoch [1/10], Step [5100/23515], Loss: 0.6137\n",
      "Epoch [1/10], Step [5120/23515], Loss: 0.7376\n",
      "Epoch [1/10], Step [5140/23515], Loss: 1.6289\n",
      "Epoch [1/10], Step [5160/23515], Loss: 0.7792\n",
      "Epoch [1/10], Step [5180/23515], Loss: 0.5046\n",
      "Epoch [1/10], Step [5200/23515], Loss: 0.2856\n",
      "Epoch [1/10], Step [5220/23515], Loss: 1.2989\n",
      "Epoch [1/10], Step [5240/23515], Loss: 1.3059\n",
      "Epoch [1/10], Step [5260/23515], Loss: 1.2780\n",
      "Epoch [1/10], Step [5280/23515], Loss: 1.1783\n",
      "Epoch [1/10], Step [5300/23515], Loss: 1.5357\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dongl\\Desktop\\aistudio\\demo3\\modify\\2_rnn.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dongl/Desktop/aistudio/demo3/modify/2_rnn.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dongl/Desktop/aistudio/demo3/modify/2_rnn.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dongl/Desktop/aistudio/demo3/modify/2_rnn.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dongl/Desktop/aistudio/demo3/modify/2_rnn.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m20\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dongl/Desktop/aistudio/demo3/modify/2_rnn.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m], Step [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dongl/Desktop/aistudio/demo3/modify/2_rnn.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         \u001b[39m.\u001b[39mformat(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, num_epochs, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(train_loader), loss\u001b[39m.\u001b[39mitem()))\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\optim\\optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    158\u001b[0m          grads,\n\u001b[0;32m    159\u001b[0m          exp_avgs,\n\u001b[0;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    162\u001b[0m          state_steps,\n\u001b[0;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m func(params,\n\u001b[0;32m    214\u001b[0m      grads,\n\u001b[0;32m    215\u001b[0m      exp_avgs,\n\u001b[0;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    218\u001b[0m      state_steps,\n\u001b[0;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\optim\\adam.py:307\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    305\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    306\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    309\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.animator import Animator\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "# ani = Animator()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "            \n",
    "            # y_true, y_pred = score(model, val_loader, device)\n",
    "            # accuracy = sum([1 if y_true[i] == y_pred[i] else 0 for i in range(len(y_true))]) / len(y_true)\n",
    "            # inter = i / len(train_loader)\n",
    "            # ani.ax.plot(epoch + inter, loss.item(), 'r.', ms=10)\n",
    "            # ani.ax.plot(epoch + inter, accuracy, 'b.', ms=10)\n",
    "            # ani.ax.set_xlabel('epoch')\n",
    "            # ani.ax.set_ylabel('loss')\n",
    "            # ani.ax.set_title('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}'\n",
    "            #                  .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item(), accuracy))\n",
    "            # ani.render(0.01)\n",
    "\n",
    "    y_true, y_pred = score(model, val_loader, device)\n",
    "    accuracy = sum([1 if y_true[i] == y_pred[i] else 0 for i in range(len(y_true))]) / len(y_true)\n",
    "    print('Epoch [{}/{}], Accuracy: {:.2f}'\n",
    "        .format(epoch+1, num_epochs, accuracy))\n",
    "    \n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# ani.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
